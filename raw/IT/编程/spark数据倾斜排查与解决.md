

## 背景

三四月份的时候写了一个判重子模块，用于对招中标的公告进行重复度判断，对外暴露接口如下：

![image-20221103102335208](https://raw.githubusercontent.com/Abug0/Typora-Pics/master/pics/202211031023239.png)

在生产环境经过一段时间的验证后，效果还可以，遂被安排对数据库里的数据做一次切库，使用子模块做去重。然后，就出现问题了..

## 切库

切库的主体是一个脚本，任务会提交到spark集群执行。去重的逻辑是：

* 将标题和公告发布日期作为key，对招中标公告进行划分；
* 对于相同key的公告，两两进行相似度判断，判定为重复的公告需要删除；
* 其中的相似度判断即是调用is_duplicate接口来完成的。

## 问题

### （一）死循环

事情的起初是负责切库的同事反馈说去重模块中可能存在死循环，让我排查一下。

询问下得知，切库过程在去重环节出现了阻塞，有task执行了12+h都没有完成（其他任务最长不超过2h），且将去重脚本回退到加入去重之前的版本后，切库流程可以顺畅完成，所以，切库同事和spark同事都怀疑是去重模块内部存在死循环导致的阻塞。

当时第一想法是觉得这种可能性比较小，一是因为当初写这段代码的时候就在尽力避免死循环，对循环条件做了严格的规定，二则是因为去重模块已在线上运行了相当一段时间，并未发现有这类问题，不过想法归想法，确实也不能排除是某种特殊数据触发了意料之外的bug，所以还是赶紧去review了几遍代码，结果并未发现可能存在的死循环问题（同事也进行了review）。

### （二）集群环境排查与火焰图分析

于是认为问题有可能出在其他地方，我最初的想法是可能spark集群环境存在问题。

找同事拿到了spark环境(web界面)，并且spark同事这个时候提供了一张关于阻塞进程的火焰图。

先是看了下火焰图：

![image-20230101192905013](https://raw.githubusercontent.com/Abug0/Typora-Pics/master/pics/Typora20230101192912.png)

从图上看，去重模块内部出现死循环的可能性基本可以排除，因为假设模块内部出现死循环的话，那么模块内部的某处调用应该在火焰图上表现为一条很宽的线，而实际上却并未出现。

所以这个时候又接着去看了下spark集群的问题，但检查log后同样并未发现相关线索，且spark集群相关权限也不在我们这里，所以暂时搁置这个思路，打算先从其他方面排查。

这时候还是想到了同事说的死循环，于是又重新找出火焰图进行分析。虽然从火焰图上看，可以排除去重模块内部死循环的可能性，但如果循环不是出现在模块内部呢？如果是死循环出现在了其它地方呢？

于是又找同事拿到了切库脚本的源码，进行review。结果发现切库中相关代码基本都是for循环，且从逻辑上分析，也并不存在死循环的可能。那么还有没有其他可能呢？

另一方面则是和spark同事沟通，希望能拿到问题进程的coredump进行分析，遗憾的是拿到的coredump无法进行分析（解释器原因），不过通过其他方式看到了问题进程的当前堆栈信息，并无异常。

重新分析火焰图，想到另一种可能就是，循环次数很大，任务在正常运行，只是耗时太久，所以让人误以为是死循环造成的阻塞。并且火焰图也支持这种假设。和同事分享了这个想法，无奈同事对此不置可否...

只能继续分析其他原因...

如果不存在阻塞，也不是死循环，但是进程还是迟迟执行未完成，那么，有没有可能是进程耗时太久？是不是库里存在一批数据造成的耗时太长？

另一边，同事则提出局部+全部聚合的方式，具体方案是在去重的前一个阶段中为每个key加一个随机数，将每个key尽可能均匀的分为n份，先对每个小key进行去重，而后再合并小key，进行第二次去重，通过这种方式，可以将耗时长的数据由一个task分配到多个task上，以此来减少去重所需时间。

但这种方式有一个缺点，就是如果小key的去重效果不好，那么在后面的全局聚合中，依然会在某个task上由大量的数据需要处理，而且由于增加了局部聚合的步骤，反而会增加总体的耗时。

不过当前重要的是先执行方案，而后根据结果再进行进一步的分析，不然空想无益。

提交代码，启动任务...

第二天，不出意料的，同样的问题还是存在...

接着讨论其他的解决方案。

从目前来看，只有一个task会出现这种问题，而且该task的输入数据在几十M这个量级，相比之下，库里的总数据大小却是几十T级的，并且库内主要是公告数据，从业务上讲，丢弃或者跳过这小部分数据几乎不存在什么影响，于是和同事讨论这个想法。

最初想法是在任务方法内部加计时器，某个key耗时超过一定时间后直接跳过，放弃对该key的去重，另外需要记录log，用于之后的分析。

无奈同事不太接受计时这种方案，于是退而求其次，改为通过循环次数判断，阈值暂时设为70w次（根据之前的性能测试数据，耗时应该不会超过4h，除非这部分数据都比较大）。

再次提交代码，启动任务...

10+h后，意料之外又情理之中的，继续挂...

不过此时通过记录的log，找到了存在问题的key...

在库里搜索相关数据，发现确实是一批比较大的数据，大约1000条左右，平均每条100+k，由此推算，即使只做70w次循环，耗时确实也是远超4h的，估计要几十h...

于是和同事定下最终方案，还是采用计时方法，耗时超过后直接跳过。

再次提交后，任务顺利完成。